{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "# Predicting the 'next word' using Naive Bayes model\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "I was asked to prepare a 4 minute presentation on Naive Bayes and I thought it would be intereseting to see whether I could use a Naive Bayes model to predict the next word that might be typed using the previous two words.\n",
    "\n",
    "The problem came down to dimensionaity. In the sklearn Naive Bayes implementation, dense matrices are produced so that the conditional probability between each pair of words is calculated. Where there are many 1000s of unique words, this would create a out of memory error.\n",
    "\n",
    "Many words can only be followed by certain other specif words. Most will have some restrictions. The idea of this page is to create a Naive Bayes model froms scratch using sparse matrices that can cope with very large dimension data.\n",
    "\n",
    "This model is never going to win any competitions but I did learn plenty putting this together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import glob as glob\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and cleaning the data\n",
    "\n",
    "The data is the Gutenburg collection of Charles Dickens' novels. The data is converted from text to a table consisting of \n",
    "3-grams.\n",
    "\n",
    "All letter are converted to lower case, and all punctation is removed. There are some editorial notes in the text documents that I am using that I have not spent any time cleaning. The word 'Gutenburg' does get predicted surprisingly often"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram=3\n",
    "def clean(corpus):\n",
    "    '''\n",
    "    Simplifies our data by removing punctuation\n",
    "    '''\n",
    "    corpus = re.sub(r'[^a-zA-Z ]+', ' ', corpus)\n",
    "    return corpus\n",
    "\n",
    "def get_data():\n",
    "    '''\n",
    "    Returns a data frame of target words with prior words\n",
    "    '''\n",
    "    data=[]\n",
    "    for book in glob.glob('./texts/*.txt'):\n",
    "        corpus = open(book, encoding='utf8').read()\n",
    "        corpus = clean(corpus)\n",
    "        corpuslist = corpus.lower().split()\n",
    "        for wordlistnumber in range(len(corpuslist)-ngram+1):\n",
    "            data.append(corpuslist[wordlistnumber:wordlistnumber+ngram])\n",
    "\n",
    "    data = pd.DataFrame(data, columns=['word-2','word-1','word'])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a data set\n",
    "data=get_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating labels and sparse matrices\n",
    "\n",
    "The idea is to hold the data in sparse matrices. The LabelEncoder creates a 1-1 mapping between words and numbers.\n",
    "\n",
    "There are 2 functions for creating sparse matrices. \n",
    "* 'prob_word' returns just the count of all words as a sparse matrix\n",
    "* cond_prob_matrix can be used to create the conditional probability (E.g. given that the word is 'a' what is the probability that the prior word was 'and'\n",
    "\n",
    "The calculation of the conditional probability occurs further down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to numbers\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "lbl = LabelEncoder()\n",
    "lbl.fit(list(data.iloc[0,:].values) + list(data.iloc[:,-1].values))\n",
    "for i in range(len(data.columns)):\n",
    "    data.iloc[:,i] = lbl.transform(data.iloc[:,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prob_word():\n",
    "    prob_word_df = data.iloc[:,-1]\n",
    "    prob_word_df = prob_word_df.groupby(prob_word_df).size()\n",
    "    prob_word_df = prob_word_df.reset_index(name='word count')\n",
    "    length = len(prob_word_df)\n",
    "    prob_word = sparse.coo_matrix((prob_word_df['word count']))\n",
    "    return prob_word\n",
    "\n",
    "def cond_prob_matrix(word_lag):\n",
    "    # makes copy of data with word and word with lag. Count number of incidents\n",
    "    cond_prob_df = data.iloc[:,[-1-word_lag,-1]]   \n",
    "    cond_prob_df = cond_prob_df.groupby(by=list(cond_prob_df.columns)).size()\n",
    "    cond_prob_df = cond_prob_df.reset_index(name='word pair count')\n",
    "    \n",
    "    # calculate the incident of each word\n",
    "    prob_word_df = data.iloc[:,-1]\n",
    "    prob_word_df = prob_word_df.groupby(prob_word_df).size()\n",
    "    prob_word_df = prob_word_df.reset_index(name='word count')\n",
    "    \n",
    "    # Merge dataframes. Calculate the conditional proabilities\n",
    "    cond_prob_df = pd.merge(cond_prob_df, prob_word_df, on='word', how='inner')\n",
    "    cond_prob_df['cond_prob'] = cond_prob_df['word pair count']/cond_prob_df['word count']\n",
    "    \n",
    "    # Create and return sparse matrix\n",
    "    cond_matrix = sparse.coo_matrix((cond_prob_df['cond_prob'], (cond_prob_df.iloc[:, 0], cond_prob_df.iloc[:, 1])))\n",
    "    return cond_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_w = prob_word()\n",
    "cond_prob1 = cond_prob_matrix(1)\n",
    "cond_prob2 = cond_prob_matrix(2)\n",
    "word = lbl.inverse_transform([x for x in range(prob_w.shape[1])])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions to get probability of each word\n",
    "\n",
    "The probability_next_word functions return 1 x n matrices that provides the probability of Word given that the two prior words were the last two words in the chain\n",
    "\n",
    "The probability uses the chain rule - namely that the: \n",
    "$P(word-1 = a,  word-2 = b) = P(word-1 = a) x P(word-2=b)$\n",
    "\n",
    "In addition, two function have been created to provide both the most likely next word, and to produce a next word at random (based on the probabilities of each word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability_next_word(chain):\n",
    "    # Create matrix of most likely next word\n",
    "    [a,b] = lbl.transform(chain[-2:])\n",
    "    x1 = cond_prob1.getrow(b) #time consuming\n",
    "    x2 = cond_prob2.getrow(a) #time consuming\n",
    "    x3 = prob_w.multiply(x1).multiply(x2).toarray().T    \n",
    "    \n",
    "    #turn data into suitable dataframe with names\n",
    "    total =x3.sum()\n",
    "    df = pd.DataFrame(x3/total, index = word, columns=['Probability'])\n",
    "    return df\n",
    "\n",
    "def random_next_word(chain):\n",
    "    df = probability_next_word(chain)\n",
    "    return np.random.choice(df.index.values, p=df['Probability'].values)\n",
    "\n",
    "def most_likely_next_word(chain):\n",
    "    return probability_next_word(chain).sort_values('Probability', ascending=False).index[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples\n",
    "\n",
    "The model predicts some sensible words based on the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'copperfield'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_next_word(['master', 'david'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'copperfield'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_likely_next_word(['master', 'david'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "master david s son what s fate for a bad un in the window accidentally broke my mother fondling me davy speak replied sam weller beguiled the way of which had the general of warm ma am i am waited for some harmless from my parents got a chariot one having the appointed of me and adams within a week or limitation permitted by u s federal laws and made the first old fox is a pretty or you want to spend the rest goes that wintry afternoon still she was dressed much soiled skeleton suit of the river anio diverted from\n"
     ]
    }
   ],
   "source": [
    "chain= ['master', 'david']\n",
    "\n",
    "for i in range(100):\n",
    "    chain.append(random_next_word(chain))\n",
    "\n",
    "    \n",
    "print(' '.join(chain))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
